# Báo cáo Tuần 2 - Lab 3: NLP Pipeline với PySpark
**Nguyễn Thùy Trang - 22000128**

## 1. Các bước triển khai

### 1.1 Chuẩn bị môi trường
- Cài đặt Python 3.11.9 và tạo môi trường ảo (venv)
- Cài đặt PySpark: `pip install pyspark`
- Chuẩn bị dữ liệu: file `c4-train.00000-of-01024-30K.json.gz`

### 1.2 Thiết kế pipeline NLP
Pipeline được thiết kế với 4 giai đoạn chính:

1. **Data Loading**: Đọc dữ liệu JSON nén từ dataset C4
2. **Tokenization**: Sử dụng RegexTokenizer để tách văn bản thành tokens
3. **Stop Words Removal**: Loại bỏ các từ dừng bằng StopWordsRemover
4. **Vectorization**: Chuyển đổi tokens thành vectors TF-IDF

### 1.3 Cài đặt code
```python
# Khởi tạo SparkSession
spark = SparkSession.builder \
    .appName("Lab3 NLP Pipeline") \
    .master("local[*]") \
    .getOrCreate()

# Đọc dữ liệu và giới hạn 1000 records
df = spark.read.json(data_path).limit(1000)

# Tạo pipeline với 4 stages
pipeline = Pipeline(stages=[tokenizer, stop_words_remover, hashing_tf, idf])
```

## 2. Cách chạy code và ghi log kết quả

### 2.1 Lệnh chạy
```bash
& "D:\1. Studying\NLP\HW1_1809_NguyenThuyTrang\.venv\Scripts\python.exe" "src/spark/lab3_pyspark.py"
```

### 2.2 Ghi log tự động
Code tự động lưu kết quả vào:
- `results/lab3_metrics.log`: Thông số hiệu suất
- `results/lab3_pipeline_output.txt`: Kết quả 20 records đầu tiên

## 3. Giải thích các kết quả thu được

### 3.1 Hiệu suất Pipeline
Từ file `lab3_metrics.log`:
- **Thời gian fit pipeline**: 1.92 giây
- **Thời gian transform dữ liệu**: 0.76 giây
- **Kích thước từ vựng thực tế**: 31,355 từ duy nhất
- **HashingTF numFeatures**: 20,000

### 3.2 Phân tích kết quả
- Pipeline xử lý thành công 1000 records từ dataset C4
- Từ vựng thực tế (31,355) lớn hơn số features (20,000) → xảy ra hash collisions
- Mỗi văn bản được chuyển thành vector TF-IDF với 20,000 chiều

### 3.3 Ví dụ kết quả cụ thể
Từ file `lab3_pipeline_output.txt`, Record 1:
- **Văn bản gốc**: "Beginners BBQ Class Taking Place in Missoula! Do you want to get better at making delicious BBQ?..."
- **TF-IDF Vector**: Vector thưa với 62 chiều có giá trị khác 0 trong tổng số 20,000 chiều
- **Đặc điểm**: Các từ quan trọng như "bbq", "class", "delicious" có trọng số TF-IDF cao

## 4. Khó khăn gặp phải và cách giải quyết

### 4.1 Vấn đề môi trường
- **Khó khăn**: Ban đầu thử dùng Scala với sbt nhưng gặp lỗi class không tìm thấy
- **Giải pháp**: Chuyển sang PySpark, dễ cài đặt và debug hơn

### 4.2 Vấn đề cấu trúc project
- **Khó khăn**: sbt không nhận diện đúng cấu trúc thư mục
- **Giải pháp**: Sử dụng Python với cấu trúc thư mục đơn giản hơn

### 4.3 Vấn đề dữ liệu
- **Khó khăn**: File dữ liệu lớn, xử lý chậm
- **Giải pháp**: Giới hạn 1000 records để tăng tốc độ trong quá trình phát triển

### 4.4 Vấn đề Hash Collision
- **Khó khăn**: Từ vựng thực tế (31,355) > numFeatures (20,000)
- **Giải pháp**: Chấp nhận trade-off giữa memory và độ chính xác, có thể tăng numFeatures nếu cần

## 5. Nguồn tham khảo

- **Apache Spark Documentation**: https://spark.apache.org/docs/latest/
- **PySpark MLlib Guide**: https://spark.apache.org/docs/latest/ml-guide.html
- **C4 Dataset**: Common Crawl dataset for language modeling
- **TF-IDF Theory**: Manning, C. D., Raghavan, P., & Schütze, H. (2008). Introduction to information retrieval

## 6. Model và công cụ sử dụng

- **PySpark MLlib**: Thư viện machine learning của Apache Spark
- **RegexTokenizer**: Tokenizer dựa trên regex pattern `\\s+|[.,;!?()\"']`
- **StopWordsRemover**: Sử dụng danh sách stop words mặc định của Spark
- **HashingTF**: Hash-based term frequency với 20,000 features
- **IDF**: Inverse Document Frequency cho weighting

Không sử dụng model pre-trained ngoài, tất cả đều là implementation từ PySpark MLlib.

## 7. Kết luận

Lab 3 đã thành công triển khai một pipeline NLP hoàn chỉnh với PySpark, bao gồm:
- ✅ Đọc dataset C4 vào Spark DataFrame
- ✅ Implement Spark ML Pipeline  
- ✅ Sử dụng RegexTokenizer cho tokenization
- ✅ Sử dụng StopWordsRemover để loại bỏ stop words
- ✅ Sử dụng HashingTF và IDF cho vectorization
- ✅ Fit pipeline và transform dữ liệu
- ✅ Lưu kết quả vào file
- ✅ Log quá trình xử lý

Pipeline có thể xử lý dữ liệu lớn hiệu quả và tạo ra các vector TF-IDF chất lượng cao cho các tác vụ NLP tiếp theo.
