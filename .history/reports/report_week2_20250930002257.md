# Báo cáo Tuần 2 - Lab 3: NLP Pipeline với PySpark
**Nguyễn Thùy Trang - 22000128**

## 1. Các bước triển khai

### 1.1 Chuẩn bị môi trường
- Cài đặt PySpark: `pip install pyspark`
- Chuẩn bị dữ liệu: file `c4-train.00000-of-01024-30K.json.gz`

### 1.2 Thiết kế pipeline NLP
Pipeline được thiết kế với 4 giai đoạn chính:

1. **Data Loading**: Đọc dữ liệu JSON nén từ dataset C4
2. **Tokenization**: Sử dụng RegexTokenizer để tách văn bản thành tokens
3. **Stop Words Removal**: Loại bỏ các từ dừng bằng StopWordsRemover
4. **Vectorization**: Chuyển đổi tokens thành vectors TF-IDF -> 

### 1.3 Cài đặt code
```python
# Khởi tạo SparkSession
spark = SparkSession.builder \
    .appName("Lab3 NLP Pipeline") \
    .master("local[*]") \
    .getOrCreate()

# Đọc dữ liệu và giới hạn 1000 records
df = spark.read.json(data_path).limit(1000)

# Tạo pipeline với 4 stages
pipeline = Pipeline(stages=[tokenizer, stop_words_remover, hashing_tf, idf])
```

## 2. Cách chạy code và ghi log kết quả

### 2.1 Lệnh chạy
Có 2 cách chạy code:

**Cách 1: Sử dụng Python environment đã cài PySpark**
```bash
python src/spark/lab3_pyspark.py
```

**Cách 2: Sử dụng virtual environment (khuyến nghị)**
```bash
# Kích hoạt virtual environment
.venv\Scripts\activate

# Chạy script
python src/spark/lab3_pyspark.py
```

**Lưu ý**: Đảm bảo đã cài đặt PySpark: `pip install pyspark`

### 2.2 Ghi log tự động
Code tự động lưu kết quả vào:
- `results/lab3_metrics.log`: Thông số hiệu suất
- `results/lab3_pipeline_output.txt`: Kết quả 20 records đầu tiên

## 3. Giải thích các kết quả thu được

### 3.1 Hiệu suất Pipeline
Từ file `lab3_metrics.log`:
- **Thời gian fit pipeline**: 1.92 giây
- **Thời gian transform dữ liệu**: 0.76 giây
- **Kích thước từ vựng thực tế**: 31,355 từ duy nhất
- **HashingTF numFeatures**: 20,000

### 3.2 Phân tích kết quả
- Pipeline xử lý thành công 1000 records từ dataset C4
- Từ vựng thực tế (31,355) lớn hơn số features (20,000) → xảy ra hash collisions
- Mỗi văn bản được chuyển thành vector TF-IDF với 20,000 chiều

### 3.3 Ví dụ kết quả cụ thể
Từ file `lab3_pipeline_output.txt`, Record 1:
- **Văn bản gốc**: "Beginners BBQ Class Taking Place in Missoula! Do you want to get better at making delicious BBQ?..."
- **TF-IDF Vector**: Vector thưa với 62 chiều có giá trị khác 0 trong tổng số 20,000 chiều
- **Đặc điểm**: Các từ quan trọng như "bbq", "class", "delicious" có trọng số TF-IDF cao

## 4. Khó khăn gặp phải và cách giải quyết

### 4.1 Vấn đề môi trường
- **Khó khăn**: Ban đầu thử dùng Scala với sbt nhưng gặp lỗi class không tìm thấy
- **Giải pháp**: Chuyển sang PySpark, dễ cài đặt và debug hơn

### 4.2 Vấn đề cấu trúc project
- **Khó khăn**: sbt không nhận diện đúng cấu trúc thư mục
- **Giải pháp**: Sử dụng Python với cấu trúc thư mục đơn giản hơn

### 4.3 Vấn đề dữ liệu
- **Khó khăn**: File dữ liệu lớn, xử lý chậm
- **Giải pháp**: Giới hạn 1000 records để tăng tốc độ trong quá trình phát triển

### 4.4 Vấn đề Hash Collision
- **Khó khăn**: Từ vựng thực tế (31,355) > numFeatures (20,000)
- **Giải pháp**: Chấp nhận trade-off giữa memory và độ chính xác, có thể tăng numFeatures nếu cần

## 5. Nguồn tham khảo

- **Apache Spark Documentation**: https://spark.apache.org/docs/latest/
- **PySpark MLlib Guide**: https://spark.apache.org/docs/latest/ml-guide.html

## 6. Model và công cụ sử dụng

- **PySpark MLlib**: Thư viện machine learning của Apache Spark
- **RegexTokenizer**: Tokenizer dựa trên regex pattern `\\s+|[.,;!?()\"']`
- **StopWordsRemover**: Sử dụng danh sách stop words mặc định của Spark
- **HashingTF**: Hash-based term frequency với 20,000 features
- **IDF**: Inverse Document Frequency cho weighting

Không sử dụng model pre-trained ngoài, tất cả đều là implementation từ PySpark MLlib.

## 7. Kết luận

Pipeline có thể xử lý dữ liệu lớn hiệu quả và tạo ra các vector TF-IDF chất lượng cao cho các tác vụ NLP tiếp theo.
