# Báo cáo Tuần 2 - Lab 3: NLP Pipeline với PySpark
**Nguyễn Thùy Trang - 22000128**

## 1. Các bước triển khai

### 1.1 Chuẩn bị môi trường
- Cài đặt PySpark: `pip install pyspark`
- Chuẩn bị dữ liệu: file `c4-train.00000-of-01024-30K.json.gz`

### 1.2 Thiết kế pipeline NLP
Pipeline được thiết kế với 4 giai đoạn chính:

1. **Data Loading**: Đọc dữ liệu JSON nén từ dataset C4
2. **Tokenization**: Sử dụng RegexTokenizer để tách văn bản thành tokens
3. **Stop Words Removal**: Loại bỏ các từ dừng bằng StopWordsRemover
4. **Vectorization**: Chuyển đổi tokens thành vectors TF-IDF -> 

### 1.3 Cài đặt code và xử lý song song
```python
# Khởi tạo SparkSession với xử lý song song
spark = SparkSession.builder \
    .appName("Lab3 NLP Pipeline") \
    .master("local[*]") \  # Sử dụng tất cả CPU cores để xử lý song song
    .getOrCreate()

# Đọc dữ liệu và giới hạn 1000 records
df = spark.read.json(data_path).limit(1000)

# Tạo pipeline với 4 stages
pipeline = Pipeline(stages=[tokenizer, stop_words_remover, hashing_tf, idf])
```

**Giải thích về xử lý song song:**
- `master("local[*]")`: Cấu hình Spark chạy trên máy local và sử dụng tất cả CPU cores
- Pipeline tự động phân phối công việc trên nhiều cores
- Mỗi stage trong pipeline được tối ưu hóa để xử lý song song

## 2. Cách chạy code và ghi log kết quả

### 2.1 Lệnh chạy

```bash
python src/spark/lab3_pyspark.py
```
-  Đảm bảo đã cài đặt PySpark: `pip install pyspark`

### 2.2 Ghi log tự động
Code tự động lưu kết quả vào:
- `results/lab3_metrics.log`: Thông số hiệu suất
- `results/lab3_pipeline_output.txt`: Kết quả 20 records đầu tiên

## 3. Giải thích các kết quả thu được

### 3.1 Hiệu suất Pipeline
Từ file `lab3_metrics.log`:
- **Thời gian fit pipeline**: 1.92 giây
- **Thời gian transform dữ liệu**: 0.76 giây
- **Kích thước từ vựng thực tế**: 31,355 từ duy nhất
- **HashingTF numFeatures**: 20,000

### 3.2 Phân tích kết quả chi tiết
- **Xử lý song song**: Pipeline tận dụng tất cả CPU cores để xử lý 1000 records đồng thời
- **Hiệu quả**: Tổng thời gian chỉ 2.68 giây (1.92s fit + 0.76s transform) cho 1000 văn bản
- **Hash collisions**: Từ vựng thực tế (31,355) lớn hơn số features (20,000) → một số từ khác nhau có thể map cùng index
- **Vector thưa**: Mỗi văn bản chỉ có ~62/20,000 chiều khác 0, tiết kiệm memory đáng kể
- **TF-IDF weights**: Từ hiếm có trọng số cao, từ phổ biến có trọng số thấp

### 3.3 So sánh với xử lý tuần tự
- **Ưu điểm của Spark**: Xử lý song song giúp tăng tốc đáng kể so với xử lý từng document
- **Scalability**: Pipeline có thể mở rộng xử lý hàng triệu documents mà không cần thay đổi code
- **Memory management**: Spark tự động quản lý memory và cache trung gian

### 3.4 Ví dụ kết quả cụ thể
Từ file `lab3_pipeline_output.txt`, Record 1:
- **Văn bản gốc**: "Beginners BBQ Class Taking Place in Missoula! Do you want to get better at making delicious BBQ?..."
- **TF-IDF Vector**: Vector thưa với 62 chiều có giá trị khác 0 trong tổng số 20,000 chiều
- **Đặc điểm**: Các từ quan trọng như "bbq", "class", "delicious" có trọng số TF-IDF cao

## 4. Khó khăn gặp phải và cách giải quyết

### Vấn đề môi trường
- **Khó khăn**: Ban đầu thử dùng Scala với sbt nhưng gặp lỗi class không tìm thấy
- **Giải pháp**: Chuyển sang PySpark, dễ cài đặt và debug hơn

###  Vấn đề cấu trúc project
- **Khó khăn**: sbt không nhận diện đúng cấu trúc thư mục
- **Giải pháp**: Sử dụng Python với cấu trúc thư mục đơn giản hơn

### Vấn đề Hash Collision
- **Khó khăn**: Từ vựng thực tế (31,355) > numFeatures (20,000)
- **Giải pháp**: Chấp nhận trade-off giữa memory và độ chính xác, có thể tăng numFeatures nếu cần

## 5. Nguồn tham khảo

- **Apache Spark Documentation**: https://spark.apache.org/docs/latest/
- **PySpark MLlib Guide**: https://spark.apache.org/docs/latest/ml-guide.html

## 6. Model và công cụ sử dụng

- **PySpark MLlib**: Thư viện machine learning của Apache Spark
- **RegexTokenizer**: Tokenizer dựa trên regex pattern `\\s+|[.,;!?()\"']`
- **StopWordsRemover**: Sử dụng danh sách stop words mặc định của Spark
- **HashingTF**: Hash-based term frequency với 20,000 features
- **IDF**: Inverse Document Frequency cho weighting

Không sử dụng model pre-trained ngoài, tất cả đều là implementation từ PySpark MLlib.

## 7. Kết luận

Lab 3 đã thành công triển khai một pipeline NLP hoàn chỉnh với PySpark, bao gồm:
- ✅ **Xử lý song song**: Tận dụng tất cả CPU cores để tăng tốc độ xử lý
- ✅ **Pipeline scalable**: Có thể mở rộng xử lý hàng triệu documents  
- ✅ **Vectorization hiệu quả**: Chuyển đổi văn bản thành vector TF-IDF chất lượng cao
- ✅ **Memory optimization**: Sử dụng vector thưa và cache tối ưu
- ✅ **Logging đầy đủ**: Ghi nhận metrics và kết quả chi tiết

Pipeline đã chứng minh khả năng xử lý dữ liệu lớn hiệu quả và tạo ra các vector TF-IDF chất lượng cao cho các tác vụ NLP tiếp theo. Việc sử dụng Spark giúp tăng tốc độ xử lý đáng kể so với các phương pháp tuần tự truyền thống.
